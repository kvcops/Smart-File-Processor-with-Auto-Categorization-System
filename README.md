# Smart File Processor (Auto Categorization System)

![AWS](https://img.shields.io/badge/AWS-Free%20Tier-orange) ![Python](https://img.shields.io/badge/Python-3.8+-blue) ![License](https://img.shields.io/badge/License-MIT-green)

## ğŸ“Œ What is this?

This project helps you organize and understand your business files automatically. It:

* Sorts files into categories like Sales, Finance, HR, etc.
* Finds useful insights (like total sales, overdue invoices).
* Works fully on AWS (free tier).
* No manual work needed after setup.

## âœ… Features

* ğŸ“ **Auto File Sorting**: Puts files into categories.
* ğŸ“Š **Smart Reports**: Shows totals, averages, trends.
* âš ï¸ **Warnings**: Alerts if data is missing or something is wrong.
* ğŸ§¹ **Data Cleaning**: Cleans data and adds timestamps.
* ğŸ“„ **Insight Reports**: Saves results in a separate folder.
* ğŸ’¸ **Free to Use**: Works in AWS free tier.

## ğŸ”§ Tools Used

* AWS Glue (for processing)
* Amazon S3 (for storage)
* IAM Role (for permissions)

---

## ğŸ§± System Diagram

```
Input Bucket (Raw Files) â”€â”€â–¶ Glue Job â”€â”€â–¶ Output Bucket (Sorted)
                                  â”‚
                                  â–¼
                        Insights Bucket (Reports)
```

---

## ğŸ“‚ Types of Files It Supports

* CSV (.csv)
* JSON (.json)
* Parquet (.parquet)

## ğŸ“ Categories

| Category  | File Examples      | It Finds...                     |
| --------- | ------------------ | ------------------------------- |
| Financial | invoices, payments | total amount, average, overdue  |
| Sales     | orders, revenue    | sales count, revenue trends     |
| Customer  | leads, contacts    | email domains, locations        |
| Inventory | products, stock    | low stock, out-of-stock alerts  |
| HR        | employees, salary  | departments, average salary     |
| Marketing | campaigns, leads   | top campaigns, conversion rates |

---

## ğŸ› ï¸ Setup Guide (Step by Step)

### 1. Create 3 S3 Buckets

* Go to AWS Console â†’ S3 â†’ Create 3 buckets:

  * `smart-processor-input-yourname`
  * `smart-processor-output-yourname`
  * `smart-processor-insights-yourname`

### 2. Create IAM Role for Glue

* Go to IAM â†’ Roles â†’ Create Role
* Choose Glue service
* Attach policy: `AWSGlueServiceRole`
* Name it: `SmartProcessorRole`

### 3. Add S3 Access to Role

* In IAM â†’ Roles â†’ SmartProcessorRole
* Click "Add inline policy"
* Use this JSON:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:*"],
      "Resource": [
        "arn:aws:s3:::smart-processor-*",
        "arn:aws:s3:::smart-processor-*/*"
      ]
    }
  ]
}
```

### 4. Create the Glue Job

* Go to AWS Glue â†’ Jobs â†’ Create job
* Choose: Spark Script Editor
* Paste the `data_quality_monitor.py` script
* Update the 3 bucket names inside the script
* Job config:

  * Name: `smart-file-processor`
  * IAM Role: `SmartProcessorRole`
  * Language: Python 3
  * Worker Type: G.1X (Free Tier)
  * Workers: 2

### 5. Upload Test Files

* Add `.csv`, `.json` or `.parquet` test files to the input bucket

### 6. Run the Job

* Go to Glue Job â†’ Click Run
* Check output and insights buckets

---

## ğŸ§ª Sample Files

You can upload test files like:

* `sample_invoice.csv`
* `sample_customers.csv`
* `sample_sales.csv`
* `sample_inventory.csv`

These will be categorized automatically and insights generated.

---

## âš™ï¸ How to Use

* ğŸŸ¢ Drop files into the input bucket
* â–¶ï¸ Run the Glue job manually or set a daily trigger
* ğŸ“ Check results in output and insights buckets

To automate it:

* Go to AWS Glue â†’ Triggers â†’ Create trigger
* Set time and connect to `smart-file-processor`

---

## ğŸ“‚ Output Folder Structure

### Output Bucket:

```
smart-processor-output-yourname/
â”œâ”€â”€ sales/
â”œâ”€â”€ financial/
â”œâ”€â”€ inventory/
â”œâ”€â”€ customer/
â”œâ”€â”€ hr/
â”œâ”€â”€ marketing/
```

### Insights Bucket:

```
smart-processor-insights-yourname/
â”œâ”€â”€ summary_report/
â”œâ”€â”€ category_reports/
â”‚   â”œâ”€â”€ sales/
â”‚   â”œâ”€â”€ inventory/
â”‚   â””â”€â”€ ...
```

---

## ğŸ’° Cost (Free Tier)

| AWS Service | Free Limit       | Our Usage      | Cost   |
| ----------- | ---------------- | -------------- | ------ |
| Glue        | 1M objects/month | < 1000 files   | \$0.00 |
| S3 Storage  | 5 GB             | \~100 MB       | \$0.00 |
| S3 Requests | 20K GET, 2K PUT  | \~500 requests | \$0.00 |

Itâ€™s free under AWS Free Tier!

---

## ğŸ› ï¸ Customize

* To add more categories, edit `categorize_file()` method
* To add custom rules, update the `extract_*_insights()` methods

---

## ğŸ” Debugging Tips

* Use Glue Logs (Job â†’ Runs â†’ Logs)
* Enable debug logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

Common Problems:

| Issue               | Cause               | Fix                |
| ------------------- | ------------------- | ------------------ |
| Job fails instantly | IAM Role missing S3 | Add S3 permissions |
| Empty results       | No files in input   | Upload files       |
| Out of memory       | Large files         | Use more workers   |

---

## ğŸ” Security Tips

* Enable MFA, encryption, and bucket policies
* Don't store sensitive data in plain text

---

## ğŸ“ˆ Boost Performance

* For large files:

```python
df.repartition(10)
df.write.parquet(output_path)
```

* For many files:

```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

---

Happy Processing! ğŸ‰
